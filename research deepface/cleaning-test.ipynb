{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vanshbansal/Desktop/FuelGrowth'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/vanshbansal/Desktop/FuelGrowth\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Performance                                          Video URL\n",
      "56      0.935800  https://fgimagestorage.blob.core.windows.net/f...\n",
      "13      0.333000  https://fgimagestorage.blob.core.windows.net/f...\n",
      "176     0.447996  https://fgimagestorage.blob.core.windows.net/f...\n",
      "33      0.218900  https://fgimagestorage.blob.core.windows.net/f...\n",
      "74      0.558200  https://fgimagestorage.blob.core.windows.net/f...\n",
      "30      0.677700  https://fgimagestorage.blob.core.windows.net/f...\n",
      "112     1.506244  https://fgimagestorage.blob.core.windows.net/f...\n",
      "122     0.855486  https://fgimagestorage.blob.core.windows.net/f...\n",
      "126     0.718521  https://fgimagestorage.blob.core.windows.net/f...\n",
      "149     1.222953  https://fgimagestorage.blob.core.windows.net/f...\n",
      "36      0.625000  https://fgimagestorage.blob.core.windows.net/f...\n",
      "254     1.503368  https://fgimagestorage.blob.core.windows.net/f...\n",
      "107     0.786823  https://fgimagestorage.blob.core.windows.net/f...\n",
      "18      0.429000  https://fgimagestorage.blob.core.windows.net/f...\n",
      "132     0.011060  https://fgimagestorage.blob.core.windows.net/f...\n",
      "79      1.439300  https://fgimagestorage.blob.core.windows.net/f...\n",
      "32      1.169500  https://fgimagestorage.blob.core.windows.net/f...\n",
      "43      0.687100  https://fgimagestorage.blob.core.windows.net/f...\n",
      "124     1.261632  https://fgimagestorage.blob.core.windows.net/f...\n",
      "0       1.106000  https://fgimagestorage.blob.core.windows.net/f...\n",
      "Duplicate video removal complete. Cleaned data saved to 'cleaned_video_urls2.xlsx'.\n",
      "Duplicate video removal complete. Cleaned data saved to 'cleaned_video_urls2.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "from imagehash import phash\n",
    "from PIL import Image\n",
    "\n",
    "# Load Excel data\n",
    "df = pd.read_excel('Assignment Data.xlsx')\n",
    "\n",
    "df = df.sample(20)\n",
    "print(df)\n",
    "df.rename(columns={'Video URL': 'url'}, inplace=True)\n",
    "\n",
    "# Directory to store temporary frames\n",
    "TEMP_DIR = \"temp_frames\"\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "# Function to download video and capture first few frames\n",
    "def get_video_frames(video_url, frame_count=5):\n",
    "    temp_video_path = os.path.join(TEMP_DIR, \"temp_video.mp4\")\n",
    "    urlretrieve(video_url, temp_video_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(temp_video_path)\n",
    "    frames = []\n",
    "    for _ in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (200, 200))  # Resize for consistent comparison\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    os.remove(temp_video_path)\n",
    "    return frames\n",
    "\n",
    "# Function to calculate hash for each frame\n",
    "def calculate_frame_hashes(frames):\n",
    "    hashes = []\n",
    "    for frame in frames:\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        hashes.append(phash(image))\n",
    "    return hashes\n",
    "\n",
    "\n",
    "# Compare videos and retain unique ones\n",
    "unique_rows = []  # To store rows corresponding to unique videos\n",
    "seen_hashes = set()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    url = row['url']\n",
    "    try:\n",
    "        frames = get_video_frames(url)\n",
    "        frame_hashes = calculate_frame_hashes(frames)\n",
    "        \n",
    "        # If all hashes are new, mark the video as unique\n",
    "        if not any(frame_hash in seen_hashes for frame_hash in frame_hashes):\n",
    "            unique_rows.append(row)  # Store the entire row\n",
    "            seen_hashes.update(frame_hashes)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {url}: {e}\")\n",
    "\n",
    "# Create a new DataFrame with unique rows\n",
    "unique_df = pd.DataFrame(unique_rows)\n",
    "\n",
    "# Save cleaned data to Excel, retaining all columns\n",
    "unique_df.to_excel('cleaned_video_urls2.xlsx', index=False)\n",
    "\n",
    "print(\"Duplicate video removal complete. Cleaned data saved to 'cleaned_video_urls2.xlsx'.\")\n",
    "\n",
    "# Cleanup\n",
    "for file in os.listdir(TEMP_DIR):\n",
    "    os.remove(os.path.join(TEMP_DIR, file))\n",
    "os.rmdir(TEMP_DIR)\n",
    "\n",
    "print(\"Duplicate video removal complete. Cleaned data saved to 'cleaned_video_urls2.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate video removal complete. Cleaned data saved to 'cleaned_video_urls.xlsx'.\n",
      "Duplicate video removal complete. Cleaned data saved to 'cleaned_video_urls.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "from imagehash import phash\n",
    "from PIL import Image\n",
    "\n",
    "# Load Excel data\n",
    "df = pd.read_excel('cleaned_video_urls.xlsx')\n",
    "#print(df)\n",
    "df.rename(columns={'Video URL': 'url'}, inplace=True)\n",
    "\n",
    "# Directory to store temporary frames\n",
    "TEMP_DIR = \"temp_frames\"\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "# Function to download video and capture first few frames\n",
    "def get_video_frames(video_url, frame_count=5):\n",
    "    temp_video_path = os.path.join(TEMP_DIR, \"temp_video.mp4\")\n",
    "    urlretrieve(video_url, temp_video_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(temp_video_path)\n",
    "    frames = []\n",
    "    for _ in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (200, 200))  # Resize for consistent comparison\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    os.remove(temp_video_path)\n",
    "    return frames\n",
    "\n",
    "# Function to calculate hash for each frame\n",
    "def calculate_frame_hashes(frames):\n",
    "    hashes = []\n",
    "    for frame in frames:\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        hashes.append(phash(image))\n",
    "    return hashes\n",
    "\n",
    "\n",
    "# Compare videos and retain unique ones\n",
    "unique_rows = []  # To store rows corresponding to unique videos\n",
    "unique_frame_hashes = []  # Store frame hashes for the first 3-5 seconds only\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    url = row['url']\n",
    "    try:\n",
    "        frames = get_video_frames(url)\n",
    "        frame_hashes = calculate_frame_hashes(frames)\n",
    "        \n",
    "        # Check if the extracted frame hashes match those of already seen videos\n",
    "        is_duplicate = any(\n",
    "            all(curr_hash == existing_hash for curr_hash, existing_hash in zip(frame_hashes, stored_hashes))\n",
    "            for stored_hashes in unique_frame_hashes\n",
    "        )\n",
    "        \n",
    "        if not is_duplicate:\n",
    "            unique_rows.append(row)  # Store the entire row\n",
    "            unique_frame_hashes.append(frame_hashes)  # Save the current video's hashes\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {url}: {e}\")\n",
    "\n",
    "# Create a new DataFrame with unique rows\n",
    "unique_df = pd.DataFrame(unique_rows)\n",
    "\n",
    "# Save cleaned data to Excel, retaining all columns\n",
    "unique_df.to_excel('cleaned_video_urls2.xlsx', index=False)\n",
    "\n",
    "print(\"Duplicate video removal complete. Cleaned data saved to 'cleaned_video_urls.xlsx'.\")\n",
    "\n",
    "# Cleanup\n",
    "for file in os.listdir(TEMP_DIR):\n",
    "    os.remove(os.path.join(TEMP_DIR, file))\n",
    "os.rmdir(TEMP_DIR)\n",
    "\n",
    "print(\"Duplicate video removal complete. Cleaned data saved to 'cleaned_video_urls.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
