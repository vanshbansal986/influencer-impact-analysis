{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vanshbansal/Desktop/FuelGrowth'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vanshbansal/Desktop/FuelGrowth'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"/Users/vanshbansal/Desktop/FuelGrowth\")\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class DataCleaningConfig:\n",
    "    url_score_data: Path\n",
    "    clean_data_temp_dir: Path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import *\n",
    "from src.utils.common import read_yaml , write_yaml , create_directories\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath=CONFIG_FILE_PATH,\n",
    "                ):\n",
    "        \n",
    "        self.config=read_yaml(config_filepath)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def get_data_cleaning_config(self)-> DataCleaningConfig:\n",
    "        \n",
    "        cleaning_config = self.config.data_cleaning\n",
    "        \n",
    "        create_directories([cleaning_config.clean_data_temp_dir])\n",
    "\n",
    "        data_cleaning_config = DataCleaningConfig(\n",
    "            url_score_data = cleaning_config.url_score_data,\n",
    "            clean_data_temp_dir = cleaning_config.clean_data_temp_dir\n",
    "        )\n",
    "\n",
    "        return data_cleaning_config\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "from imagehash import phash\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "from imagehash import phash\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from src import logger\n",
    "\n",
    "# Directory to store temporary frames\n",
    "\n",
    "\n",
    "\n",
    "class DataCleaning:\n",
    "    def __init__(self,config:DataCleaningConfig):\n",
    "        self.TEMP_DIR = \"temp_frames\"\n",
    "        \n",
    "        os.makedirs(self.TEMP_DIR, exist_ok=True)\n",
    "        try:\n",
    "            self.config = config\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    # Function to download video and capture first few frames\n",
    "   \n",
    "    def get_video_frames(self , video_url, frame_count=5):\n",
    "        \n",
    "        \n",
    "        temp_video_path = os.path.join(self.TEMP_DIR, \"temp_video.mp4\")\n",
    "        urlretrieve(video_url, temp_video_path)\n",
    "\n",
    "        cap = cv2.VideoCapture(temp_video_path)\n",
    "        frames = []\n",
    "        for _ in range(frame_count):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (200, 200))  # Resize for consistent comparison\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        os.remove(temp_video_path)\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "  \n",
    "    def calculate_frame_hashes(self , frames):\n",
    "        hashes = []\n",
    "        for frame in frames:\n",
    "            image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            hashes.append(phash(image))\n",
    "        return hashes\n",
    "    \n",
    "    def initiate_data_cleaning(self):\n",
    "\n",
    "        excel_data_path = self.config.url_score_data\n",
    "        \n",
    "        clean_data_temp_dir = self.config.clean_data_temp_dir\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        filename = f\"{current_time}.xlsx\"\n",
    "\n",
    "        clean_data_path = os.path.join(clean_data_temp_dir, filename)\n",
    "\n",
    "        # Load Excel data\n",
    "        logger.info(f\"Reading excel file at path: {excel_data_path}\")\n",
    "        df = pd.read_excel(excel_data_path)\n",
    "        \n",
    "        logger.info(f\"excel file at path: {excel_data_path} read Successfully !!!\")\n",
    "        \n",
    "        df.rename(columns={'Video URL': 'url'}, inplace=True)\n",
    "\n",
    "        # Compare videos and retain unique ones\n",
    "        unique_rows = []  # To store rows corresponding to unique videos\n",
    "        seen_hashes = set()\n",
    "\n",
    "        logger.info(\"Cleaning the URLs...\")\n",
    "        for _, row in df.iterrows():\n",
    "            url = row['url']\n",
    "            try:\n",
    "                frames = self.get_video_frames(url)\n",
    "                frame_hashes = self.calculate_frame_hashes(frames)\n",
    "\n",
    "                # If all hashes are new, mark the video as unique\n",
    "                if not any(frame_hash in seen_hashes for frame_hash in frame_hashes):\n",
    "                    unique_rows.append(row)  # Store the entire row\n",
    "                    seen_hashes.update(frame_hashes)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video {url}: {e}\")\n",
    "\n",
    "        # Create a new DataFrame with unique rows\n",
    "        unique_df = pd.DataFrame(unique_rows)\n",
    "\n",
    "        logger.info(f\"Saving clean data to file : {clean_data_path} ...\")\n",
    "        # Save cleaned data to Excel, retaining all columns\n",
    "        unique_df.to_excel(clean_data_path, index=False)\n",
    "        logger.info(f\"clean data at path : {clean_data_path} saved successfully !!!\")\n",
    "\n",
    "        \n",
    "\n",
    "        # Cleanup\n",
    "        for file in os.listdir(self.TEMP_DIR):\n",
    "            os.remove(os.path.join(self.TEMP_DIR, file))\n",
    "        os.rmdir(self.TEMP_DIR)\n",
    "\n",
    "        return clean_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-01 18:12:48,740: INFO: common: yaml file: config.yaml loaded successfully]\n",
      "[2024-12-01 18:12:48,742: INFO: common: created directory at: temp_data/url_score data/clean]\n",
      "[2024-12-01 18:12:48,743: INFO: 2382737675: Reading excel file at path: temp_data/url_score data/raw/Assignment Data.xlsx]\n",
      "     Performance                                          Video URL\n",
      "39      1.989600  https://fgimagestorage.blob.core.windows.net/f...\n",
      "263     0.711425  https://fgimagestorage.blob.core.windows.net/f...\n",
      "261     0.109745  https://fgimagestorage.blob.core.windows.net/f...\n",
      "260     0.037619  https://fgimagestorage.blob.core.windows.net/f...\n",
      "215     0.325333  https://fgimagestorage.blob.core.windows.net/f...\n",
      "164     0.400420  https://fgimagestorage.blob.core.windows.net/f...\n",
      "186     1.469749  https://fgimagestorage.blob.core.windows.net/f...\n",
      "12      1.402500  https://fgimagestorage.blob.core.windows.net/f...\n",
      "72      1.541000  https://fgimagestorage.blob.core.windows.net/f...\n",
      "203     1.579724  https://fgimagestorage.blob.core.windows.net/f...\n",
      "[2024-12-01 18:12:48,800: INFO: 2382737675: excel file at path: temp_data/url_score data/raw/Assignment Data.xlsx read Successfully !!!]\n",
      "[2024-12-01 18:12:48,803: INFO: 2382737675: Cleaning the URLs...]\n",
      "[2024-12-01 18:13:09,925: INFO: 2382737675: Saving clean data to file : temp_data/url_score data/clean/2024-12-01_18-12-48.xlsx ...]\n",
      "[2024-12-01 18:13:09,941: INFO: 2382737675: clean data at path : temp_data/url_score data/clean/2024-12-01_18-12-48.xlsx saved successfully !!!]\n",
      "Duplicate video removal complete. Cleaned data saved to 'cleaned_video_urls.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_cleaning_config = config.get_data_cleaning_config()\n",
    "    data_cleaning = DataCleaning(config=data_cleaning_config)\n",
    "    clean_data_path = data_cleaning.initiate_data_cleaning()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
